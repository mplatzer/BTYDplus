---
title: "Scoring your Customer Base with BTYDplus"
author: "Michael Platzer"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Scoring your Customer Base with BTYDplus}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: bibliography.bib
---

```{r, echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
library(BTYDplus)
```

# Introduction

The BTYDplus package provides advanced statistical methods to describe and predict customers' purchase behavior. It uses historic transaction records to fit probabilistic models, which then allows to compute quantities of managerial interest. 

The challenge of this task is threefold: For one, the churn event in a non-contractual customer relationship is not directly observable, but rather needs to be infered indirectly based on observed periods of inactivity. Second, with customers behaving differently, yet having oftentimes only few transactions recorded so far, we require statistical methods that can utilize cohort-level patterns as priors for estimating customer-level quantities. And third, we attempt to predict a future, that hasn't been seen before, thus need assumptions regarding the future dynamics.

Fitting one of these probabilistic models then allows you to describe a customer cohort in terms of its heterogeneous distribution of purchase patterns and dropout probabilities. More importantly, it allows to estimate the total number of future transactions, a crucial data point for capacity and production planning. Further, one can also assess the total (expected) value of a cohort, and if put in relationship to its acquisition costs, this provides quantified support for making decisions on the customer acquisition channel. 

All of these models provide us also with estimates on customer level. Thus, we can easily enrich our customer base with individual level estimates on a customer's status, future activity and future value. Such scores can be then utilized to adapt services, (marketing) messages and offers to a customer's most likely state and value. With the ease and speed of the provided models, practitioners could consider scoring their customer base with these advanced statistical techniques on a continuous base.

## Models

The [BTYD package](https://cran.r-project.org/package=BTYD) already provides implementations for the **Pareto/NBD** (@schmittlein1987cyc), the **BG/NBD** (@fader2005cyc) and the **BG/BB** (@fader2010customer) model. BTYDplus complements that package by providing several additional buy-till-you-die models, that have been published in the marketing literature, but whose implementation are complex and non-trivial. In order to create a consistent experience of users of both packages, we attempt to follow the API conventions of BTYD where possible.

The models provided as part of BTYDplus are:

* **NBD** - @ehrenberg1959pattern
* **MBG/NBD** - @batislam2007empirical
* **(M)BG/CNBD-k** - @platzer2017mbgcnbd
* **Pareto/NBD (HB)** - @ma2007mcmc
* **Pareto/NBD (Abe)** - @abe2009counting
* **Pareto/GGG** - @platzer2016pggg

The number of provided models begs the question, which one to use, and which one works best in your case. There is no simple answer to that, so you might want to try out all of these, assess data fit as well as forecast accuracy based on an artificially withheld time period and then make a tradeoff between calculation speed, data fit and accuracy. Still, we will attempt to provide some guidance on the model selection process. 

The implementation of the original *NBD* model from 1959 serves mainly as a basic benchmark. It assumes a heterogenous purchase process, but doesn't account for the possibility of customers churning. The *Pareto/NBD* model, introduced in 1987, then combined the NBD model for transactions of active customers with a heterogeneuos dropout process, and to this date still serves as a gold standard for buy-till-you-die models. The *BG/NBD* model then modified the assumptions regarding the dropout process, and turns out to generally acchieve similar levels of data fit and forecast accuracy, all while being much faster in terms of computation times when compared to Pareto/NBD. However, the BG/NBD model makes the particular assumption that every customer without a repeat transaction has *not* churned yet, independent of the elapsed time of inactivity. This seems counterintuitive, particular when compared to customers with repeat transactions, and thus the *MBG/NBD* has been developed to eliminate this inconsistency by allowing for dropout also for customers without any activity. Data fitting capabilities and forecast accuracy are comparable to BG/NBD, yet it comes with more plausible estimates for the dropout process. The recently developed *BG/CNBD-k* and *MBG/CNBD-k* model classes allow both for regularity within the transaction timings. If such regularity is present (even in a mild form), these models can yield significant improvements in terms of data fit and forecasting accuracy, while coming at the same order in terms of computational costs.

All aforementioned models benefit from closed-form solutions for key expressions and thus can be efficiently estimated via means of maximum likelihood estimation (MLE). However, this tightly restricts the model builder from relaxing any of the underlying behavioral assumptions. An alternative estimation method though is via Markov-Chain-Monte-Carlo (MCMC) simulation. MCMC comes at significantly higher costs in terms of implementation complexity and computation time, but it allows for more flexible assumptions and we simultaneously gain the benefits of (1) estimated marginal posterior distributions rather than point estimates, (2) individual-level parameter estimates, and thus (3) straightforward simulations of customer-level metrics of managerial interest. The hierarchical bayes variant of Pareto/NBD (i.e., *Pareto/NBD (HB)*) served as a proof-of-concept for the MCMC approach, but doesn't yet take advantage of the gained flexibility, as it sticks to the original Pareto/NBD assumptions. In contrast, *Pareto/NBD (Abe)* relaxes the independence of purchase and dropout process, plus it can incorporate custoemr covariates for estimating these. Particularly the latter can turn out to be very powerful, if any of such known covariates helps in explaining the heterogeneity within the customer cohort. Finally, the *Pareto/GGG* is also a generalization of Pareto/NBD, but allows for a varying degree of regularity within the transaction timings (but doesn't allow for customer covariates). Analog to (M)BG/CNBD-k, incorporating regularity can yield significant improvements in forecast accuracy, if such regularity is present in the data.

## Workflow

The analysis process of an existing customer cohort starts out by collecting a complete log of all transactions. The minimum required fields are a customer ID (column `cust`) and a date or timestamp (`date` or `t`). In addition we can capture a monetary component for each transaction (`sales`), if we later on want to estimate future sales and customer lifetime value.

```{r}
elog <- cdnow.sample()$elog
head(elog)
```

```{r, echo=FALSE, results='asis'}
knitr::kable(head(elog, 10))
```

```{r, fig.show='hold'}
plotSampledTimingPatterns(elog, n = 25)
plotSampledTimingPatterns(elog, n = 25)
```

# Maximum Likelihood Estimated Models

## (M)BG/CNBD-k

# MCMC Estimated Models

## Pareto/NBD (HB)

## Pareto/NBD (Abe)

## Pareto/GGG

# References
